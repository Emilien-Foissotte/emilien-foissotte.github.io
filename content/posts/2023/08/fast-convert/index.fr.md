---
title: "De Parquet vers CSV en un clin d'oeil"
description: ""
date: 2023-08-26T09:50:14+02:00
publishDate: 2023-08-26T09:50:14+02:00
draft: false
frtags: [DataEng, Craftmanship, DuckDB, Data]
ShowToc: true
TocOpen: false
---

# TL;DR

Dans ce post nous verrons ensemble comment convertir super efficacement et rapidement ğŸš€
des fichiers au format `Apache Parquet` vers le format `CSV` (et vice-versa), en
utilisant DuckDB ğŸ¦† et en comparant avec Pandas ğŸ comme une base de comparaison.

En bonus, nous verrons comment l'utiliser avec un format CLI super ergonomique et efficace, toujours
Ã  portÃ©e de main ğŸ‘¨â€ğŸ’»

C'est parti !

## Intro

Ces derniers temps j'ai eu de plus en plus l'occasion de travailler sur des tÃ¢ches relevant plutÃ´t du
Data Engineering pur et dur, avec par exemple des mises en place de Datalake AWS, des conversions de donnÃ©es en tout
genre, des designs de pipelines, du nettoyage de donnÃ©es.. ğŸ“Š

Et Ã©videmment, je n'ai pas Ã©tÃ© Ã©pargnÃ© par la multitude de format existants, j'ai souvent eu Ã  convertir des
donnÃ©es vers le format `.csv` pour m'assurer que le dÃ©chiffrage s'est bien dÃ©roulÃ©, que mon nettoyage a fonctionnÃ©, etc.. ğŸ‘€

Malheureusement, il serait peu efficace de travailler constamment avec des formats `.csv`, car mÃªme s'ils sont
extrÃªmement pratiques pour lire un morceau de donnÃ©e trÃ¨s rapidement, ils sont trÃ¨s gourmand en mÃ©moire de stockage.
Pour essayer d'Ã©conomiser des coÃ»ts de stockage et traitements, il est bien plus adÃ©quat d'utiliser un format de stockage de
type _colonnes_ comme `Apache Parquet`. âš¡

Et donc me revoilÃ  Ã  lancer encore et encore des commandes dans tous les sens, crÃ©er des environnements virtuels
pour installer `pandas` et `pyarrow` pour convertir mes fichiers `.parquet`.. J'ai donc tout naturellement cherchÃ©
un outil, idÃ©alement en CLI, qui me permet de faire ces conversions rapides. Et malheureusement, rien qui ne
fasse cela sur Ã©tagÃ¨re, du moins pas Ã  ma connaissance aprÃ¨s un rapide Ã©cumage des ressources de la communautÃ©
Data Engineering, quel dommage.. ğŸ¤”

C'est reparti pour les commandes dans tous les sens..

![cat](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbWVxbHpwc2FzNHh2anFtcDRoaXdianJhOGl5bDFwYXJsdW5pNHVzbyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/JIX9t2j0ZTN9S/giphy.gif#center)

Quel rÃªve d'avoir un outil qui permettrait de visualiser les donnÃ©es en un coup d'oeil, malheureusement
la rÃ©alitÃ© avec les fichiers `.parquet` est tout autre.. ğŸ˜¥

```sh
â¯ head file.parquet
B%R1x,<I/

!
b
B5
 u
:!-
<M=
P(-    bG
```

Et oui, il n'y a pas de repas gratuit, pour reprendre les termes du _no free lunch theorem_, [NFL](https://en.wikipedia.org/wiki/No_free_lunch_theorem)
pour les intimes, on ne peut pas avoir un format directement optimisÃ© pour le stockage et l'accÃ¨s en lecture,
il faut faire un choix. ğŸ¤‘

Et puis, il y a quelques temps, m'est revenu Ã  l'esprit un post d'un Data Engineer faisant la promotion de DuckDB,
et repartageant un snippet de code montrant comment cet outil peut faire ce genre d'opÃ©ration super efficacement !

Ã€ vos claviers, on va essayer de reproduire et installer Ã§a sur notre machine ! ğŸš€

## Rapide comparaison d'outils - Pandas vs DuckDB

Donc quelques mois plus tÃ´t, je tombe sur Linkedin sur ce [post](https://www.linkedin.com/posts/motherduck_csv-to-parquet-using-duckdb-cli-activity-7043982478671306752-z2EK?utm_source=share&utm_medium=member_desktop)
qui montre qu'avec un script en une ligne de commande comment DuckDB peut faire la conversion, d'un CSV vers un parquet.

Ce n'est pas l'opÃ©ration que je fais le plus mais Ã§a devrait Ãªtre possible de faire l'inverse,
comparons dÃ©jÃ  les performances entre les outils et le procÃ©dÃ©, comparativement Ã  `pandas+pyarrow` par exemple.

TÃ©lÃ©chargeons d'abord un premier dataset de taille moyenne, par exemple le [MovieLens 25M dataset](https://grouplens.org/datasets/movielens/)

Nous nous en servirons dans un prochain post, orientÃ© ML cette fois, stay tuned ! ğŸ˜‰

```sh
â¯ head -n 3 ratings.csv
userId,movieId,rating,timestamp
1,296,5.0,1147880044
1,306,3.5,1147868817
```

Comparaons avec pandas et pyarrow d'installÃ©s, nous allons voir oÃ¹ se situe les performances de ce nouvel outil
dans ce genre d'opÃ©rations.

Histoire de garder une trace et pouvoir reproduire la situation
voici un extrait d'un `pip freeze` sur mon environnement virtuel :

```sh
â¯ pip freeze
numpy==1.25.2
pandas==2.1.0
pyarrow==13.0.0
python-dateutil==2.8.2
pytz==2023.3.post1
six==1.16.0
tzdata==2023.3
```

Les performances sont les suivantes :

```sh
â¯ /usr/bin/time -l -h -p python -c "import pandas; df=pandas.read_csv('ratings.csv'); df.to_parquet('ratings.parquet')"
real 14,43
user 9,40
sys 2,80
          1774600192  maximum resident set size
                   0  average shared memory size
                   0  average unshared data size
                   0  average unshared stack size
             1037341  page reclaims
                5284  page faults
                   0  swaps
                   0  block input operations
                   0  block output operations
                   0  messages sent
                   0  messages received
                   0  signals received
                 864  voluntary context switches
               25149  involuntary context switches
         77809521962  instructions retired
         42836239895  cycles elapsed
          2854670336  peak memory footprint
```

Maintenant lanÃ§ons DuckDB ğŸ¦†

Pour installer cet outil, suivez cette [doc](https://duckdb.org/docs/installation/), c'est trÃ¨s simple :

Pour macOS

```sh
brew install duckdb
```

Et pour Linux (ciblez la bonne architecture)

```sh

curl -SL https://github.com/duckdb/duckdb/releases/download/v0.8.1/duckdb_cli-linux-amd64.zip -o /tmp/duckdb.zip
unzip /tmp/duckdb.zip
mv /tmp/duckdb/* /usr/local/bin/
chmod +x /usr/local/bin/duckdb
```

LanÃ§ons la conversion de ce fichier `.csv` ğŸš€ :

```sh
â¯ /usr/bin/time -l -h -p duckdb -c "COPY (select * from read_csv_auto('ratings.csv')) TO 'ratings.parquet' (FORMAT PARQUET)"
100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
real 9,12
user 24,58
sys 2,36
           603959296  maximum resident set size
                   0  average shared memory size
                   0  average unshared data size
                   0  average unshared stack size
              551447  page reclaims
                1741  page faults
                   0  swaps
                   0  block input operations
                   0  block output operations
                   0  messages sent
                   0  messages received
                   0  signals received
                 761  voluntary context switches
               34911  involuntary context switches
        135618259891  instructions retired
         96669139421  cycles elapsed
           645996544  peak memory footprint
```

Nous pouvons maintenant comparer les performances entre les outils !

![data](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExNXNlbDl1dnZnMzVyMHN5MTF5cHQ3MnN1ZXowNXc4NmEzYW9kbnhxZCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LaVp0AyqR5bGsC5Cbm/giphy.gif#center)

DÃ©jÃ , une premiÃ¨re impression : La barre de prgression est top,
toujours agaÃ§ant d'attendre un process sans savoir quand il va finir ni s'il tourne effectivement ou non.

NÃ©anmoins il semblerait que le fichier `.parquet` produit par DuckDB
soit lÃ©gÃ¨rement moins compressÃ© que celui par Pandas, (sÃ»rement des options que je n'ai pas activÃ©) :

```sh
â¯ du -h ratings*.parquet
225M    ratings_duckdb.parquet
168M    ratings_pandas.parquet
```

Par contre, et Ã§a c'est un gros point en faveur de DuckDB, quand on regarde `peak memory footprint`, la
mÃ©moire RAM maximale utilisÃ©e, on voit que Pandas charge l'entiÃ¨retÃ© du fichier csv en mÃ©moire quand
DuckDB fait le traitement par lots. Sur des systÃ¨mes avec des spÃ©cs limitÃ©es (Raspberry, fonctions Lambda),
cela peut Ãªtre limitant.

Le pic de mÃ©moire a une valeur `4.4` fois moins importante avec DuckDB comparativement Ã  Pandas. Excellent !

![duck](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExcWUzOGJ0OGpjanJuajg2MTkyemRxY3FqdWV1emRrdmE3cmN5bHNrZCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/x4bgmvMlRSYRVcTm29/giphy.gif#center)

## DÃ©veloppement d'un outil CLI - de maniÃ¨re Ã©lÃ©gante

Il nous suffit de modifier le `.zshrc` ou `.bashrc` avec ces petites fonctions :

```sh
function csv_to_parquet() {
    file_path="$1"
    duckdb -c "COPY (SELECT * FROM read_csv_auto('$file_path')) TO '${file_path%.*}.parquet' (FORMAT PARQUET);"
}

function parquet_to_csv() {
    file_path="$1"
    duckdb -c "COPY (SELECT * FROM '$file_path') TO '${file_path%.*}.csv' (HEADER, FORMAT 'csv');"
}
```

Et on peut charger et transformer les fichiers en une ligne de commande :

```sh
parquet_to_csv file.parquet
```

et boom, on obtient notre `file.csv` ğŸ’¥

Pour faire l'inverse, rien de plus simple :

```sh
csv_to_parquet file.csv
```

Et voici notre fichier `file.parquet`, si rapide et efficace ! ğŸï¸

## Conclusion

Nous avons parcouru trÃ¨s rapidement cette fonctionnalitÃ© super sympa de DuckDB, mais avons ce petit
exemple on voit tout la puissance de cet outil trÃ¨s polyvalent. En quelques lignes
de CLI vous avez dÃ©jÃ  un bref aperÃ§u des capacitÃ©s de DuckDB, cela vous fera gagner beaucoup
de temps dans vos tÃ¢ches de Data Engineering !

N'hÃ©sitez pas Ã  rajouter vos fonctions `.bashrc` et `.zshrc` super pratiques
en commentaires, toujours cool d'en apprendre de nouvelles !

â¤ï¸ Ã  l'Ã©quipe de DuckDB team pour le boulot incroyable !

### Liens et rÃ©fÃ©rences

- [Le post Linkedin](https://www.linkedin.com/posts/motherduck_csv-to-parquet-using-duckdb-cli-activity-7043982478671306752-z2EK?utm_source=share&utm_medium=member_desktop)
- [Repost de Mehdi Ouazza, un Data Eng Ã  suivre !](https://www.linkedin.com/posts/mehd-io_csv-to-parquet-using-duckdb-cli-activity-7043984992632229888-7GJr?utm_source=share&utm_medium=member_desktop)
- [DuckDB documentation](https://duckdb.org/docs/installation/)
- [Discover DuckDB PDF](https://duckdb.org/pdf/SIGMOD2019-demo-duckdb.pdf)
