---
title: "Cr√©er un Dashboard + Data pipeline ETL de A √† Z, et √©conomiser de l'argent"
cover:
  image: "cover.png"
  alt: "Carburoam Front Page"
  relative: true
date: 2024-06-09T18:53:22+0200
publishDate: 2024-06-09T18:53:22+0200
draft: false
tags: ["DataEng", "Python", "Streamlit", "Data"]
ShowToc: true
TocOpen: false
---

# TL;DR

Dans ce billet de blog, nous allons voir comment cr√©er de A √† Z
un projet de Data engineering, de l'ETL, la cr√©ation de notre
sch√©ma de donn√©es, l'ORM de l'application, son backend et ensuite
son d√©ploiement avec Streamlit Cloud ‚öôÔ∏è

Le but est de r√©cup√©rer la liste des prix de stations essence en France ‚õΩ,
automatiser un job qui va venir mettre √† jour les valeurs quotidiennement üìÖ et construire un dashboard pour
afficher les prix personnalis√©s aux utilisateurs du site üìä

Apr√®s la lecture de ce billet de blog, vous aurez les bases pour construire des dashboard data
et pour scrapper vos propres sources de donn√©es pour les exposer üöÄ

Simple lecteur sans v√©ll√©it√© de dev ? Vous pourrez √©conomiser √† la pompe avec le dashboard ü§ë Et r√©investir le
reste pour la [transition √©cologique](https://green-got.com/) (gagnez 1 mois gratuit avec le code `emilien-foissotte`) üòá

C'est parti !

## Intro

Je ne prends pas souvent ma voiture (l'avantage du r√©seau francilien de transports en commun), mais quand je dois le faire,
il me vient toujours un grand dilemme. Avec la multiplicit√© des stations essences autour de chez moi, et la volatilit√©
des prix √† la pompe ces derniers temps, comment choisir syst√©matiquement la moins ch√®re..? ü§®

En France üá´üá∑ nous avons l'opportunit√© d'avoir des APIs publiques maintenues par des services gouvernementaux.
Malheureusement, le site web ne fait qu'exposer les prix et ne permet pas de sauvegarder ses stations pr√©f√©r√©es. üò≠

√Ä chaque fois que je devais faire le plein, il fallait alors venir lister les prix de chaque station sur le site,
pas vraiment optimis√© pour les mobiles en plus.. üòì

Il y a quelques ann√©es, apr√®s avoir Dockeriz√© 2 / 3 choses sur ma Raspberry PI √† la maison, et m'√™tre fait un peu la main
sur Flask, j'avais expos√© un site tr√®s tr√®s minimal qui affichait les prix. Le backend √©tait un peu sale (tout √©tait hardcod√©),
mais efficace. Par contre aucun moyen de faire √©voluer l'app.. üíÄ

Quand je montrais l'app √† mes proches et mes amis, r√©action syst√©matique de leur part : "Trop bien, je peux mettre mes stations √† moi ?"
Et moi de leur r√©pondre, "Ah, euh non d√©sol√©, je ne peux mettre que les miennes pour l'instant" (√† 2 doigts de l√¢cher
un "fais un ticket stp", d√©formation professionelle üòá)..
Et ce n'√©tait pas par manque de volont√©, mais litt√©ralement le code √©tait trop monolitique pour faire √©voluer quoi
que ce soit, √† mon grand d√©sarroi.. ü´†

L'id√©e a donc germ√©e, mon nouvel objectif de projet √©tait tr√®s clair : cr√©er un dashboard expos√© sur le web, en open source, avec
uniquement du free-tier. Ce dashboard devait pouvoir √™tre accessible par n'importe qui, et on devait pouvoir y cr√©er son compte,
g√©rer ses listes de stations et surtout faire des √©conomies sur le carburant. ‚õΩ

_PS : D'ailleurs, l'√©nergie la moins ch√®re reste toujours celle qu'on ne consomme pas. Prenez votre v√©lo ou votre paire de jambes
d√®s que possible, c'est bon pour votre corps, votre porte-monnaie, votre esprit et pour la plan√®te !_ üå±

{{<figure src="frontpage.png" caption="La page d'accueil du dashboard" >}}

La version live est dispo ici [https://carburoam.streamlit.app/](https://carburoam.streamlit.app/) ! üöÄ

## Extraire les donn√©es Open Data des prix en France

Pour commencer, comme dans tout projet de Data Engineering et de BI,
la pierre angulaire du travail √† effectuer r√©side dans nos donn√©es.

Cette donn√©e doit √™tre disponible, et de qualit√©. Travaillons en premier sur cet aspect.

Pour scrapper et r√©cup√©rer notre donn√©e, i.e. les prix √† la pompe, nous utiliserons la nouvelle
plateforme d'Open Data mise en oeuvre pas les sercives gouvernementaux fran√ßais. Un flux quotidien y est maintenu,
et de relativement bonne qualit√©.

![francais](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExamRud3FicTkxdnloZHRkMWt1MXo4bms5bXcwcmo1NDQyeHc2aXZ3ZyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/KcFV1Hm2vFMAolcHbu/giphy-downsized.gif#center)

Le format est relativement simple, cela ne consiste pas en une API Rest comme rencontr√© classiquement, mais en un fichier
XML expos√© sur une URL. Toutes les donn√©es de prix, de localisation et de dates de mises √† jour y sont contenues.

Voici ci-dessous un extrait du fichier afin d'illustrer le format de donn√©e :

```xml
<?xml version="1.0" encoding="ISO-8859-1" standalone="yes"?>
<pdv_liste>
<pdv id="91190012" latitude="4870300" longitude="212800" cp="91190" pop="R">
    <adresse>27 AV DU GENERAL LECLERC</adresse>
    <ville>Gif-sur-Yvette</ville>
    <horaires automate-24-24="1">
      <jour id="1" nom="Lundi" ferme="1"/>
      <jour id="2" nom="Mardi" ferme="1"/>
      <jour id="3" nom="Mercredi" ferme="1"/>
      <jour id="4" nom="Jeudi" ferme="1"/>
      <jour id="5" nom="Vendredi" ferme="1"/>
      <jour id="6" nom="Samedi" ferme="1"/>
      <jour id="7" nom="Dimanche" ferme="1"/>
    </horaires>
    <services>
      <service>Station de gonflage</service>
      <service>Carburant additiv<E9></service>
      <service>Automate CB 24/24</service>
    </services>
    <prix nom="Gazole" id="1" maj="2024-05-08 12:30:00" valeur="1.739"/>
    <prix nom="SP98" id="6" maj="2024-05-11 13:15:00" valeur="1.999"/>
    <rupture nom="E10" id="5" debut="2024-05-10 16:16:02" fin="" type="temporaire"/>
  </pdv>
</pdv_liste>
```

Le premier point qui saute aux yeux d'un Data Eng aguerri sera la bonne nouvelle concernant la mani√®re
de repr√©senter les donn√©es de stations ! Elles y sont toutes list√©es par un objet XML bien d√©fini, `pdv` (l'acronyme de
point de vente), qui se paie le luxe d'avoir un indentifiant unique. Cela est d'un bon pr√©sage pour la r√©conciliation
de donn√©e √† chaque update, m√™me si rien ne pr√©sume quant √† une stabilit√©e esp√©r√©e du sch√©ma de donn√©e.

C'est d'ailleurs le c√¥t√© n√©gatif de la repr√©sentation par fichier, avec une API et une version, les Standard OpenAPI permettent
de facilement voir les √©volutions. Ici ce sera quand le script hurlera d'erreurs dans tous les sens (au mieux) quand les
incompatibilit√©s nous appara√Ætrons..

Deuxi√®me point rassurant, la latitude et la longitude des stations sont fournies ! C'est parfait afin de pouvoir
repr√©senter les stations sur une carte. Pas besoin de s'emb√™ter avec du G√©o-encodage, ou de la r√©conciliation d'adresse.
J'ai toujours trouv√© les cartes plus intuitives dans ces cas-l√†.

Enfin, nous retrouvons l'objet principal de ce que nous cherchons dans le champ `prix` :

- Une liste de prix pour les types de carburants fournis par la station, ainsi que la date de derni√®re mise √† jour.
- √âgalement la liste des services propos√©s par la station (lavage, automate 24/24). √Ä noter, mais √† priori nous ne nous en servirons pas dans le projet. Laissons √ßa pour un autre developpeur qui pourra d√©velopper 'Find My CarWash' :)

Construisons d√©sormais des entit√©s et des associations entre elles, sur la base de ce que nous avons pu trouver
dans ce fichier de donn√©es. Nous y ajouterons les informations contextuelles dont nous avons besoin dans le cadre du projet :
des utilisateurs par exemple..

### Gestion de la base d'utilisateurs

Pour g√©rer les utilisateurs, la cr√©ation de comptes avec mots de passe, d'emails, nous d√©finirons une table tr√®s simple.
Elle va ne contenir que les emails, noms et pseudos des utilisateurs. Tous les d√©tails de chiffrement, de token d'authentification JWT
seront manag√©s par une libraire externe au r√©f√©rentiel de donn√©e de l'application : [Streamlit-Authenticator](https://github.com/mkhorasani/Streamlit-Authenticator).

L'id√©e sera simplement de refl√©ter les utilisateurs r√©f√©renc√©s par cette librairie, et d'ajouter ceux-ci √† la table mentionn√©e pr√©c√©demment.
Pour √©viter de manipuler en base des choses sensibles comme des mots de passe (m√™me chiffr√©s avec un syst√®me conventionel de hash et de salt),
aucune association ne sera faite dans la base de donn√©e. Appliquons l'id√©e de _least priviledge_, et ici rien n'indique le besoin d'avoir
l'acc√®s aux mots de passe.

Malheureusement, dans la mani√®re d'utiliser cette libraire, le fonctionnement classique exposerait chaque utilisateur √†
un risque de s√©curit√©.
En effet, en cas d'oubli de mot de passe, la seule option qui est propos√©e consiste √† r√©initialiser le mot de passe. Donc n'importe qui
pourrait demander la r√©-initialisation de celui-ci, sans l'aval de l'utilisateur concern√©..

![password](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExNG1lc2Q0ZjY5azV5aHQzcm9vZWpxZzFsdWdrcHRnMDZiN3dieXh1aCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l0G17mcoGBEabVgn6/giphy.gif#center)

Afin de coontourner ce risque, l'id√©e mise en place a √©t√© d'utiliser le syst√®me de mail pour envoyer un code v√©rification
envoy√© par mail √† l'utilisateur. Si ce code correspond √† ce qui a √©t√© envoy√©, alors il peut recevoir un nouveau mot de passe par le
biais de cette adresse mail.

Le stockage de cette logique applicative sera effectu√© dans une table d√©di√©e, contenant les dates de g√©n√©ration (afin de g√©rer l'expiration),
ainsi que les codes de v√©rification envoy√©s.

### G√©rer les stations essences recens√©es et les prix

Pour permettre aux utilisateurs de cr√©er une liste de stations essence personnalis√©es, une table `Custom_station` sera d√©riv√©e de la
`Stations`.
Chaque instance de l'objet `Price` sera associ√© √† une station. Et un prix aura √©videmment son association avec un type de carburant.
C'est √† dire qu'il pourra √™tre d√©fini comme un diesel, un essence SP95, E10, E85, GPLc...

Pour que les utilisateurs puissent choisir quels types de carburant les int√©ressent, une table d'association doit √™tre d√©clar√©e.
Elle marque le lien au niveau de l'ORM `SQLAlchemy` entre un utilisateur et un type de carburant, de la table `gas_types`

Ci-dessous, un diagramme avec toutes les entit√©s et les associations permet de r√©sumer les objets de notre mod√®le de donn√©e.
Les types d'association (1:1 / Many to One) ne sont pas repr√©sent√©s, mais l'ajout des cl√©s primaires et √©trang√®res est indiqu√©.
Cela devrait √™tre suffisant pour lire le diagramme et comprendre les principales relations.

![EA](EA_GasStations.png#center)

### D√©clarer ces entit√©s dans un ORM

Afin d'utiliser facilement toutes ces tables, nous allons d√©clarer les classes `SQLAlchemy` et les relier entre elles. Pour faire
cela nous allons utiliser la version 2 de cette librairie Python, avec l'impl√©mentation d√©clarative.

Voici la d√©claration de ces diff√©rentes classes, sous le module `models.py` de notre application

```python
from typing import List

import sqlalchemy as sa
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship


class Base(DeclarativeBase):
    pass


association_table = sa.Table(
    "association_table",
    Base.metadata,
    sa.Column("gastype_id", sa.ForeignKey("gas_types.id"), primary_key=True),
    sa.Column("user_id", sa.ForeignKey("users.id"), primary_key=True),
)


class GasType(Base):
    __tablename__ = "gas_types"
    # id = sa.Column(sa.Integer, primary_key=True)
    id: Mapped[int] = mapped_column(primary_key=True)
    xml_id = sa.Column(sa.String, nullable=False)
    name = sa.Column(sa.String, nullable=False)
    users: Mapped[List["User"]] = relationship(
        secondary=association_table, back_populates="gastypes"
    )

    def __repr__(self):
        return f"<GasType {self.name}>"


class User(Base):
    __tablename__ = "users"
    # id = sa.Column(sa.Integer, primary_key=True)
    id: Mapped[int] = mapped_column(primary_key=True)
    email = sa.Column(sa.String, unique=True, nullable=False)
    username = sa.Column(sa.String, unique=True, nullable=False)
    name = sa.Column(sa.String, nullable=False)
    # add a reference to the stations
    stations = relationship("CustomStation")
    # add a reference to the gas types followed
    gastypes: Mapped[List["GasType"]] = relationship(
        secondary=association_table, back_populates="users"
    )

    def __repr__(self):
        return f"<User {self.username}>"

    def to_dict(self):
        return {
            "id": self.id,
            "email": self.email,
            "username": self.username,
            "name": self.name,
        }

    def to_csv(self):
        return f"{self.id},{self.email},{self.username},{self.name}"


class VerificationCode(Base):
    __tablename__ = "verification_codes"
    id = sa.Column(sa.Integer, primary_key=True)
    user_id = sa.Column(sa.Integer, sa.ForeignKey("users.id"), nullable=False)
    code = sa.Column(sa.String, nullable=False)
    created_at = sa.Column(sa.DateTime, nullable=False)

    def __repr__(self):
        return f"<VerificationCode {self.id}>"


class Station(Base):
    __tablename__ = "stations"
    id = sa.Column(sa.Integer, primary_key=True)
    latitude = sa.Column(sa.Float, nullable=False)
    longitude = sa.Column(sa.Float, nullable=False)
    town = sa.Column(sa.String, nullable=False)
    address = sa.Column(sa.String, nullable=False)
    zip_code = sa.Column(sa.String, nullable=False)
    sa.Index("latitude_longitude_index", latitude, longitude, unique=True)

    def __repr__(self):
        return f"<Station {self.id}>"

    def to_dict(self):
        return {
            "id": self.id,
            "latitude": self.latitude,
            "longitude": self.longitude,
            "town": self.town,
            "address": self.address,
            "zip_code": self.zip_code,
        }


class CustomStation(Base):
    __tablename__ = "custom_stations"
    id = sa.Column(sa.Integer, sa.ForeignKey("stations.id"), primary_key=True)
    user_id = sa.Column(sa.Integer, sa.ForeignKey("users.id"), nullable=False)
    custom_name = sa.Column(sa.String, nullable=False)

    def __repr__(self):
        return f"<CustomStation {self.id}-{self.user_id}>"

    def to_dict(self):
        return {
            "id": self.id,
            "user_id": self.user_id,
            "custom_name": self.custom_name,
        }


class Price(Base):
    __tablename__ = "prices"
    gastype_id = sa.Column(sa.Integer, sa.ForeignKey("gas_types.id"), primary_key=True)
    station_id = sa.Column(sa.Integer, sa.ForeignKey("stations.id"), primary_key=True)
    updated_at = sa.Column(sa.DateTime, nullable=False)
    price = sa.Column(sa.Float, nullable=False)
```

Bien ! Et comment utiliser ces diff√©rentes classes dans notre application Streamlit ?

Rien de bien compliqu√©, il nous suffit d'instancier un objet de type `Session` !

Voici comment d√©clarer cela sous le module `session.py` :

```python
import logging
from functools import lru_cache
from typing import Generator

import sqlalchemy
import streamlit as st
from sqlalchemy import create_engine
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy_utils import create_database, database_exists

from models import Base, GasType

logger = logging.getLogger("gas_station_app")
engine = create_engine("sqlite:///db.sqlite3", pool_pre_ping=True)


@lru_cache
def create_session() -> scoped_session:
    """Create a session given the url in settings."""
    Session = scoped_session(
        sessionmaker(autocommit=False, autoflush=False, bind=engine)
    )
    return Session


def get_session() -> Generator[scoped_session, None, None]:
    """Retrieve a session."""
    Session = create_session()
    try:
        yield Session
    finally:
        Session.remove()


database_creation = False
db_session = create_session()
logger.info("session created")
created_engine = db_session.bind
if not database_exists(created_engine.url):
    logger.info("Database does not exist, creating it")
    create_database(created_engine.url)
    database_creation = True
Base.metadata.bind = engine
Base.metadata.create_all(bind=created_engine)


### initialize the database with mandatory data


def create_gastypes(db_session):
    """
    Create the gas types in the database.

    Args:
        db_session: sqlalchemy session

    Returns:
        None
    """
    logger.info("Creating gas types")
    gas_dict = {"Gazole": 1, "SP95": 2, "SP98": 6, "E85": 3, "GPLc": 4, "E10": 5}

    for name, xml_id in gas_dict.items():
        if not db_session.query(GasType).filter(GasType.name == name).first():
            gas_type = GasType(name=name, xml_id=xml_id)
            db_session.add(gas_type)
    try:
        db_session.commit()
    except sqlalchemy.exc.IntegrityError:
        db_session.rollback()


if database_creation:
    create_gastypes(db_session=db_session)
```

Ces diff√©rentes lignes peuvent para√Ætre complexes, mais si on les relis s√©quentiellement, c'est tr√®s simple
√† comprendre.
Au d√©marrage de l'application, le module est initialis√© par Python (puisqu'il est import√© par `home.py`, le main de
notre application).

Une instanciation de session est donc effectu√©e :
`engine = create_engine("sqlite:///db.sqlite3", pool_pre_ping=True)` va venir cr√©er le moteur de l'ORM
sqlalchemy.

Comme nous allons utiliser un cache LRU, les appels seront moins fr√©quents √† la base SQLite, les objets en cache seront
r√©utilis√©s par les diff√©rents appels de l'application. Python r√©utilisera le m√™me objet de sortie de la fonction
`get_session` plusieurs fois, jusqu'√† expiration du cache.

D√©sormais, int√©ressons nous √† cette √©trange fonction `create_gastypes`. Que fait-elle ?

Si SQLAlchemy d√©tecte que la base SQLite est vide, sans les tables du sch√©ma, alors le moteur ORM va d√©clencher la cr√©ation de ces tables et
du sch√©ma associ√© dans le module `models.py`.
Pour fonctionner correctement, notre table `gas_types` doit √™tre aliment√©e avec la donn√©e du r√©f√©rentiel de l'[API Open Data](https://www.prix-carburants.gouv.fr/rubrique/opendata/).

Ici aucun moyen de r√©cup√©rer ces identifiants de mani√®re automatique, il va falloir hardcoder ces valeurs, et prier pour que cela n'√©volue pas sans pr√©venir
dans le temps..

![specs](specs.png#center)

Et c'est tout ! Tous les autres modules pourront effectuer des appels √† l'objet `db_session` de ce module, et
le tour est jou√© üöÄ

Notre DataWarehouse/Entrep√¥t de donn√©e est pr√™t √† recevoir la donn√©e de notre ETL, penchons-nous maintenant sur ce
bloc d'architecture.

_NB: Ici je ne couvrirais pas les √©l√©ments de Streamlit-Authenticator, ceux-ci sont tr√®s bien
illustr√©s dans la documentation GH du package, allez y jeter un oeil, c'est bien expliqu√© !_

## Rafra√Æchissement quotidien de la donn√©e

R√©sumons ce dont nous disposons d√©sormais :

- Un workspace Streamlit gratuit, qui peut r√©cup√©rer quotidiennement de la donn√©e et d√©poser les valeurs dans une DB SQLite
- Un fichier d'export expos√© depuis une API Open Data publique
- Une UI √† l'adresse [carburoam.streamlit.app](https://carburoam.streamlit.app/) qui ne peut qu'exposer l'application Streamlit
  (il faut malheureusement abandonner l'id√©e de pouvoir y brancher un `airflow`, `dagster` et compagnie..)

Donc o√π est cach√© notre ETL ici ? En effet, il nous manque une pi√®ce centrale dans notre projet de Data Engineering : l'outil
d'orchestration des flux de traitements de donn√©e !

Si nous pouvions avoir une instance d'airflow quelque part, alors assur√©ment nous pourrions r√©pondre √† ce probl√®me avec
ce genre d'outillage, mais il faut faire une croix dessus ici..

Constuisons alors quelque chose de plus simple. Ce ne sera pas tr√®s r√©silient, mais √† l'√©chelle du projet, consid√©rons que ce
sera amplement suffisant.

### Orchestrateur de jobs en pur Python

Pour d√©clencher nos jobs, nous allons uniquement nous baser sur le process Python principal, et cr√©er un child-process, afin de d√©clencher tout
le m√©canisme d'update.
Ce sous-process va contenir uniquement un Thread avec un timer, qui va d√©clencher quotidiennement une t√¢che afin de :

1. Extraire le fichier depuis l'API Open Data
2. It√©rer sur les items export√©s depuis le fichier pour les charger en base

![update](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExY3R5dXFuMjI1MjI5ZXlrb3phazQydmg0cDFleGN3NWpucHJhM3dnbyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/j3WJjjm1OKV73l6E6e/giphy.gif#center)

Techniquement, on pourrait presque appeler √ßa un b√©b√© "Hello World" de job CRON. Voyons
comment cr√©er pas √† pas, cet √©l√©ment :
{{<mermaid>}}
flowchart TD
U[User] -->|Load Landing Page| L{Streamlit app}
L -->|Le fichier pid.txt </br> existe| PE[D√©clencher le subprocess]
L -->|Le fichier pid.txt n'</br> existe pas| PN[Lire la derni√®re date d'ex√©cution]
PN -->|Le fichier lastjob.txt </br> existe| LE[V√©rifier la date courante]
LE -->|delta entre la derni√®re ex√©cution </br> moins que le seuil| LT[Ne pas d√©clencher le subprocess]
LE -->|delta entre la derni√®re ex√©cution </br> plus que le seuil| MT[D√©clencher le subprocess </br> et suppression des logs plus vieux que la veille]
PN -->|le fichier lastjob.txt n' </br> existe pas| LN[D√©clencher le subprocess </br> et suppression des logs plus vieux que la veille]
{{</mermaid>}}

Est-ce que cela suffit ? √Ä peu pr√®s, oui !
Nous aurions pu utiliser la DB afin de stocker ces √©v√©nements, cela aurait simplement demand√© d'avoir un watcher Python
qui viendrait requ√™ter la table p√©riodiquement. Pour √©viter une surcharge de la DB, une simple gestion par fichier
remplace les marqueurs de completion.
Nos 2 marqueurs se font avec :

- `pid.txt` : contient le PID du job d'ETL avec le thread et son timer
- `lastjob.txt` : contient la date de derni√®re ex√©cution

De cette fa√ßon, la base de donn√©e ne sera pas surcharg√©e durant les d√©veloppements. Un re-d√©ploiment du code source
de l'application ne d√©clenchera pas de job d'ETL suppl√©mentaire si la derni√®re date est r√©cente.

Si un probl√®me appara√Æt lors de l'ETL, nous pouvons tuer le subprocess via son PID et en d√©marrer un nouveau,
en supprimant le fichier `pid.txt`.

Afin d'assurer un minimum de maintenabilit√© durant le debugging, les flux stdout et stderr du script seront
redirig√©s vers un fichier text, sous le dossier `outputs`.

```python
import logging
import os
import subprocess
import sys
import uuid
from datetime import datetime
from pathlib import Path

import streamlit as st

from utils import WAIT_TIME_SECONDS

logger = logging.getLogger("gas_station_app")


def trigger_etl():
    """
    Trigger the ETL process in a subprocess.
    """
    # create a new uuid for process opening
    str_uuid = str(uuid.uuid4())
    with open(f"outputs/stdout_{str_uuid}.txt", "wb") as out, open(
        f"outputs/stderr_{str_uuid}.txt", "wb"
    ) as err:
        subprocess.Popen([f"{sys.executable}", "utils.py"], stdout=out, stderr=err)
    if not os.path.exists("pid.txt"):
        logger.info("No pid file found, creating one")
        # if it doesn't exist, trigger the subprocess job
        # delete and remove output files under outputs
        for file in Path("outputs").glob("*.txt"):
            # get last modified date
            try:
                last_modified = datetime.fromtimestamp(file.stat().st_mtime)
                # if the file is older than 1 day, remove it
                if (datetime.now() - last_modified).days > 1:
                    logger.info(f"Removing {file}")
                    file.unlink()
            except FileNotFoundError:
                # it means another process has deleted the file
                pass

        if os.path.exists("lastjob.txt"):
            # check the last job date, do not start subprocess if recent
            with open("lastjob.txt", "r") as file:
                date = file.read()
                # parse the date (dumped as datetime.now())
                date = datetime.strptime(date, "%Y-%m-%d %H:%M:%S.%f")
                st.session_state["lastjob"] = date
                # if the detla from now is greater than WAIT_TIME_SECONDS
                if (datetime.now() - date).total_seconds() > WAIT_TIME_SECONDS:
                    logger.info("Last job was not recent, starting new job")
                    trigger_etl()
                else:
                    logger.info("Last job was recent, skipping")
        else:
            trigger_etl()
    if os.path.exists("lastjob.txt"):
        with open("lastjob.txt", "r") as file:
            date = file.read()
            date = datetime.strptime(date, "%Y-%m-%d %H:%M:%S.%f")
            st.session_state["lastjob"] = date
```

Avec la donn√©e de derni√®re ex√©cution en cache, nous pouvons aussi avoir une bonne m√©trique affichant la date de dernier job d'extraction

![metric_date](metric_date.png#center)

_NB: L'√©l√©ment sys.executable est tr√®s important pour s'assurer que l'executable Pyhton utilis√©
est le m√™me que celui utilis√© par l'application Streamlit, celle de l'environnement virtuel o√π toutes
les d√©pendances y sont install√©s. Utiliser directement `python` pourrait causer des bugs_

Qu'en est-il de l'impl√©mentation du thread avec timer ?

Plut√¥t simple aussi :

```python
import os
import signal
from threading import Timer
import threading

WAIT_TIME_SECONDS = 60 * 60 * 6  # each 6 hours


class ProgramKilled(Exception):
    pass


def signal_handler(signum, ffoorame):
    raise ProgramKilled


class Job(threading.Thread):
    def __init__(self, interval, execute, *args, **kwargs):
        threading.Thread.__init__(self)
        self.daemon = False
        self.stopped = threading.Event()
        self.interval = interval
        self.execute = execute
        self.args = args
        self.kwargs = kwargs

    def stop(self):
        self.stopped.set()
        self.join()

    def run(self):
        while not self.stopped.wait(self.interval.total_seconds()):
            self.execute(*self.args, **self.kwargs)


def main_etl():
    print("Running ETL job at ", datetime.now())
    # print the process pid
    print("Process ID: ", os.getpid())
    with open("lastjob.txt", "w") as file:
        file.write(str(datetime.now()))
    loadXML()
    dump_stations()


def etl_job():
    # check if status file exists
    if not os.path.exists("pid.txt"):
        with open("pid.txt", "w") as file:
            file.write(str(os.getpid()))
        # start etl at beginning of the thread
        main_etl()
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        job = Timer(WAIT_TIME_SECONDS, main_etl)
        job.start()

        while True:
            try:
                time.sleep(1)
            except ProgramKilled:
                print("Program killed: running cleanup code")
                # remove the pid file
                if os.path.exists("pid.txt"):
                    os.remove("pid.txt")
                job.cancel()
                break
    else:
        print("PID file already found, job as already started. Exiting...")
        exit(1)

if __name__ == "__main__":
    etl_job()
```

La routine principale est impl√©ment√©e sous la fonction `etl_job` qui √©tait pr√©c√©demment import√©e.
Ici, une double v√©rification est utilis√©e, pour v√©rifier que le fichier PID n'a pas d√©j√† √©t√© cr√©√© (avec la
concurrence, une multitude d'utilisateurs pourrait essayer de lancer la page d'accueil en m√™me temps).

Les "signals handlers" permettent de faire un nettoyage des √©l√©ments (le `pid.txt` notamment) lorsque le signal SIGTERM est re√ßu du
process parent. C'est √† dire que d√®s que l'application est arr√™t√©e, alors le thread lancera le nettoyage.

Ensuite, nous d√©marrons le timer et lan√ßons une boucle infinie, jusqu'√† qu'une Exception soit lev√©e par le
"signal handler".
De cette mani√®re, le script supprime le fichier PID.

![cleanup](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExYzdtM2RvcHc2ZTlza2RkeGk3eDI1ZTVndHRvb2IwNDF4c3ZsZWs5aiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l2Je9c6EJAuE7mYMM/giphy.gif#center)

Parfait ! Nous avons d√©sormais notre syst√®me d'ETL pour charger en base les
diff√©rents prix expos√©s sur l'API gouvernementale.

Construisons un dashboard ergonomique, afin que les utilisateurs puissent :

- Consulter la page principale, avec une liste des prix tri√©s par stations et valeurs, ainsi que la redirection vers les autres pages
- Se connecter au site web, recevoir leur mot de passe, nom d'utilisateur automatiquement s'ils l'ont oubli√©
- Modifier, Ajouter et Supprimer leur profile utilisateur, en leur donnant le contr√¥le total dessus (RGPD compliant) sur les champs
  utilisateurs (email, nom) et les types de carburants pr√©f√©r√©s.
- Choisir de nouvelles stations √† ajouter/supprimer de leurs stations favorites

En bonus :

- Une page d'informations, pour montrer un guide d'utilisation
- Une barre de navigation verticale qui donne un style tr√®s professionnel, gr√¢ce √† une base inspir√©e de cette <cite> app open source[^1]</cite>,
  avec l'UI de la barre verticale.

[^1]:
    Une [app](https://github.com/SiddhantSadangi/pdf-workdesk) d√©velopp√©e par Siddhant Sadangi, un streamlit Granmaster. Allez jeter un oeil √† ses autres applications
    sur Github, elles sont trop cools !

## Fabrication et design de l'UI

### Page d'accueil

La page d'accueil doit se comporter diff√©rement le statut de l'utilisateur, selon si l'utilisateur est logg√© ou non logg√©.
Sur la base de cette connaissance, l'UX doit √™tre diff√©rente.
Voyons √©tape par √©tape comment construire une UX coh√©rente.

#### Utilisateurs non logg√©s

Pour les nouveaux arrivants, les id√©es principales sont :

1. Fournir en premi√®re intention une mani√®re de cr√©er un compte sur la plateforme
   ![welcome](app_ui/welcome.png#center)
2. Proposer de voir une d√©mo du dashboard. Personnellement, je ne me vois pas cr√©er un compte sur quelque chose dont je ne peux pas voir la valeur
   ajout√©e (achat, personnalisation ou autre valeur ajout√©e par une app !). Ajouter une page d√©mo me semble donc un vrai bonus, si ce n'est quasi-obligatoire !
   ![demo1](app_ui/demo_1.png#center)
   ![demo2](app_ui/demo_2.png#center)
3. Montrer un page explicative √† propos de l'application, et laisser l'utilisateur comprendre, explorer un peu les insights de l'application.
   Pour les d√©veloppeurs, c'est aussi ici que le lien vers le repo Open Source est dispo !
   ![about](app_ui/about.png#center)

#### Utilisateurs connect√©s

Pour les utilisteurs qui ont cr√©√© un compte et qui se sont connect√©s sur l'application,
si je me mets √† leur place, j'aimerais (par ordre de priorit√©):

1. Pouvoir rapidement rep√©rer les prix de mes stations enregistr√©es, et identifier le prix le moins cher
   ![pricelist](app_ui/pricelist.png#center)
2. √ätre capable de savoir de quand date le rafra√Æchissement du prix pour chaque station, et de l'extract complet
   ![lastdate](app_ui/lastdate.png#center)
3. Me faire une id√©e de ce que me permet de gagner l'utilisation du dashboard, pour engager √† l'utiliser (nudge)
   ![savings](app_ui/savings.png#center)
4. Rapidement avoir un tour d'horizon de ce que me permet de faire le dashboard en termes de customisation de profil, stations..etc
   ![pages](app_ui/pages.png#center)

### Console d'administration

Pour les besoins d'administration, la console de l'application web doit pouvoir fournir :

5. Quelques donn√©es √† propos du traffic de l'app, son taux d'utilisation, d'adoption
   ![admin](app_ui/admin.png#center)
6. Des options de gestion d'op√©rations pour les utilisateurs, pour les besoins de support (r√©initialisation de
   mot de passe, refresh des jobs ETL et monitoring des logs locaux, possibilit√© de lancer du code shell)
   ![admin_actions_1](app_ui/admin_actions_1.png#center)
   ![admin_actions_2](app_ui/admin_actions_2.png#center)

Gr√¢ce au syst√®me d'authentification, il est possible de restreindre l'acc√®s √† ce dashboard uniquement √† l'utilisateur
_admin_. Ainsi, aucune donn√©e sensible ne pourra √™tre exfiltr√© par un utilisateur malicieux.

En packagant toutes ces pages, les diff√©rents modules utilitaires pour les jobs d'ETL, et en
d√©ployant le code sur un repo Github, nous avons une application Streamlit en live sur le cloud !

Tout est host√© par Streamlit, aucune prise de t√™te !

![tree](tree.png#center)

## Garder l'application active et le fichier DB local dans la dur√©e

Comme nous l'avons revu auparavant, la database SQLite va bootstrapper les diff√©rentes instances de `Users`
sauvegard√©es dans le fichier d'authentification, sauvegard√© sur le stockage AWS S3.

Par contre, les instances `Stations`, `Price` et `CustomStation` ne sont visibles que depuis la DB SQLite.
Il y a donc un risque, si jamais nous perdons le fichier SQLite (en cas de reboot de l'environnement Streamlit par exemple, il
est explicitement mentionn√© que Streamlit ne garantit pas la persistance des DB locales).

Pour √©viter que l'application tombe en mode veille et que l'orchestrateur de Streamlit d√©commissionne intempestivement le conteneur,
la VM ou le serveur o√π tourne l'application, il faut assurer un traffic minimal sur l'app pour emp√™cher cette √©ventualit√©.

![sleeping](sleeping.png#center)

Afin d'√™tre au dessus du seuil minimal d'une visite toutes les 48H (seuil actuel d√©fini par Streamlit),
j'ai emprunt√© et modifi√© une Github Action d'une autre application tr√®s tr√®s cool, d√©velopp√©e par <cite>Jean Milpied[^2]</cite>
un Data Scientist Fran√ßais √©galement !

Voici le fichier YAML Github Action :

```yaml
name: Trigger Probe of Deployed App on a CRON Schedule
on:
  schedule:
    - cron: "0 */48 * * *"

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  build-and-probe:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Build Docker Image
        run: docker build -t my-probe-image -f probe-action/Dockerfile .

      - name: Run Docker Container
        run: docker run --rm my-probe-image
```

L'action qui va sonder si l'application est off ou non est un script JavaScript,
ex√©cut√© par Puppeteer:

```js
const puppeteer = require("puppeteer");
const TARGET_URL = "https://carburoam.streamlit.app/";
const WAKE_UP_BUTTON_TEXT = "app back up";
const PAGE_LOAD_GRACE_PERIOD_MS = 8000;

console.log(process.version);

(async () => {
  const browser = await puppeteer.launch({
    headless: true,
    ignoreHTTPSErrors: true,
    args: ["--no-sandbox"],
  });

  const page = await browser.newPage();
  console.log(page); // Print the page object to inspect its properties

  await page.goto(TARGET_URL);

  console.log(page); // Print the page object to inspect its properties

  // Wait a grace period for the application to load
  await page.waitForTimeout(PAGE_LOAD_GRACE_PERIOD_MS);

  const checkForHibernation = async (target) => {
    // Look for any buttons containing the target text of the reboot button
    const [button] = await target.$x(
      `//button[contains(., '${WAKE_UP_BUTTON_TEXT}')]`,
    );
    if (button) {
      console.log("App hibernating. Attempting to wake up!");
      await button.click();
    }
  };

  await checkForHibernation(page);
  const frames = await page.frames();
  for (const frame of frames) {
    await checkForHibernation(frame);
  }

  await browser.close();
})();
```

Le d√©clenchement est effectu√© via une image Docker de Puppeteer :

```Dockerfile
# probe-action/Dockerfile
FROM ghcr.io/puppeteer/puppeteer:17.0.0
COPY ./probe-action/probe.js /home/pptruser/src/probe.js
ENTRYPOINT [ "/bin/bash", "-c", "node -e \"$(</home/pptruser/src/probe.js)\"" ]
```

[^2]:
    Une autre app sympa, donnant des informations √† la sauce BI avec les probabilit√©s de r√©parer son appareil
    domestique. L'app a r√©cemment eu raison sur ma derni√®re machine √† espresso manuelle üòÉ Jetez un oeil
    [ici](https://github.com/JeanMILPIED/reparatorAI/tree/main/probe-action) pour le d√©clenchement de l'app, et
    vous pourrez creuser le blog originel de l'id√©e ici initialement cr√©e par
    [David Young](https://dcyoung.github.io/post-streamlit-keep-alive/).

Avec cette technique, toutes les 48 heures, le script va venir sonder l'application √† l'url fournie (la page d'accueil) et
donc s'assurer que l'app est up (sinon il va cliquer sur le boutton _Wake Up_)

Actuellement, le seul √©l√©ment qui manque √† la version actuelle serait la possiblit√© de faire des d√©ploiement en
Green/Blue (avec une base tampon qui charge les anciennes donn√©es, pendant que la base couramment utilis√©e apr√®s
le refresh n'est pas trop sollicit√©e par un hard refresh).
En effet, vous l'aurez compris, actuellement si l'application crash ou bien que le serveur est d√©commissionn√© (
ce qui est probable, j'image que Streamlit doit mettre les app sur des SpotInstances pour ne pas faire exploser la facture
). J'ai d√©j√† exp√©riment√© plusieurs reboot de l'app depuis son d√©ploiement en phase _beta_, donc c'est un risque.

En effet, on perd la DB, mais instaurer un syst√®me Green/Blue va bien au del√† de la stack technique actuelle, c'est un peu
overkill pour le projet.
Je me note seulement pour le futur de faire un petit script de restauration sauvegarde et de restauration de la DB depuis son dernier
bootstrap pour le futur. Avec le S3 cela ne devrait pas √™tre bien sorcier, et surtout que nous ne souhaitons garder que les
`CustomStation` et les `Followed GasTypes`.

## Conclusion

Streamlit est un outil extr√™mement polyvalent, qui donne la possiblit√© de

- Crafter une petite application, avec une UI rapidement sympa, et √† peu pr√®s responsive, et d√©j√† en soit c'est dingue.
- Avec quelques hacks, on peut construire un ETL basique pour rafra√Æchir automatiquement la donn√©e qui est expos√©e. A
  ne pas consid√©rer comme un syst√®me de PROD, c'est un peu cracra, mais √ßa marche et √ßa tourne √† peu pr√®s. Pour un petit
  projet fun, c'est amplement suffisant.

Dans tout projet Data, assurez-vous d'avoir un sch√©ma de donn√©e coh√©rent et √©volutif. Construire ensuite votre front et la
logique de l'application n'en sera que diablemenet plus simple et efficace.

En plus d'avoir des objets d'ORM √©l√©gants..

Un grand ‚ù§Ô∏è aux diff√©rents d√©veloppeurs qui ont open-sourc√© leurs apps([pdf-workdesk](https://pdfworkdesk.streamlit.app/),
[reparatorAI](https://reparatorai.streamlit.app/), librairies ([Streamlit-Authenticator](https://github.com/mkhorasani/Streamlit-Authenticator/)),
sans eux le travail aurait √©t√© plus complexe, voir m√™me, disons-le impossible. Allez leur mettre des üåü, √ßa leur fera le plus grand plaisir !

Il en va de m√™me aux √©quipes de Streamlit et la possiblit√© de d√©ployer gratuitement nos dashboards. Merci !
